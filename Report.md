# Report

This project is based on source code offered by the course:[`deep-reinforcement-learning/ddpg-bipedal`](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal).

## State and Action Space










## State and Action Space

The simulation contains a single agent that navigates a large environment. At each time step, it has four actions to choose from:
- `0` - walk forward 
- `1` - walk backward
- `2` - turn left
- `3` - turn right

The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction:

- Values 1-36 ray values
- Value 37 is the agents velocity


## Learning algorithm

The program is structured in the following way:

- `Navigation.ipynb` - Notebook with additional information about the program structure
- `agent_dqn.py` - There the DQN class is defined, which will be used in the notebook
- `model_dqn.py` - Defines the DQN neural network for local and target network
- `README.md` - Instructions how to setup this repository and start the agent
- `Report.md` - This file
- `rewards_plot.png` - Plot of the rewards per episode for the best training
- `solved_model_weights.pth` - Saved model weights (pytorch)
- `unity-environment.log` - wasn't me. autogenerated file from unity environment

The modules are structured like this, because I am going to extend the code with the rainbow algorithm. This way I can plug the needed modules in and out as I need them.

The control structure is defined within the `dqn` function in the navigation notebook and will use functions from the agent instance from the agent class defined in `agent_dqn.py`. The agent class will use the neural network architecture defined in `model_dqn.py`. The `agent_dqn.py` contains two neural networks (local/target) to learn the action values. The target network will be updated every `UPDATE_EVERY` steps.

The training will end under the following circumstances:
- the environment is solved
- until `n_episodes` are reached
- `max_t` time steps are reached.
The environment is considered solved when the average reward (over the last 100 episodes) is at least +13.

I built in a trendline function for further tests to see what maximum score I get. The trendline will check the last 100 episodes for the trend (increasing or decreasing scores). In case of a decreasing or stable trend, the training will end. For submission I disabled/commented this function - I want to do further tests after submission.

A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. It seems, that the environment won't spawn new bananas, because raising `max_t` more than special amount, won't lower the episodes for solving the environment.

After a successful training, the weight parameters of the network are saved for later reuse (chapter 4 of the jupyter notebook).


## Hyper Parameters  

### RL Parameters

- `n_episodes (int)`: maximum number of training episodes
- `max_t (int)`: maximum number of timesteps per episode
- `eps_start (float)`: starting value of epsilon, for epsilon-greedy action selection
- `eps_end (float)`: minimum value of epsilon
- `eps_decay (float)`: multiplicative factor (per episode) for decreasing epsilon

Just to remember:
- `epsilon` = greedy factor for exploration vs. exploitation
- `alpha` = something like learnrate. defines the step size when an update occurs
- `gamma` = discount factor for rewards

Setup:
`n_episodes=2000`, `max_t=50000`, `eps_start=0.5`, `eps_end=0.01` and`eps_decay=0.97`

Search method for Hyper Parameters:

First I started almost with the same parameters as lunar lander. Except for `eps_start`. I lowered `eps_start` to 0.5 to get a balance between exploration vs. exploitation, like suggested in a former video. In the beginning I expected the agent to act more randomly (due additional random initialization of the neural network).

In the beginning I used the `eps_decay` from Lunar Lander, but lowered the decay to get a faster converge of the algorithm. I lowered the epsilon decay faster with an higher epsilon greedy decay so the greedy factor will come in sooner. I decreased the values slowly (by 0.1), so on lower episodes the score increased faster. At higher decay rate (lower than 0.97) the training got unstable going lower as 0.96, so I stopped.

I set an higher number of timesteps `max_t`, because the performance is measured against the episodes only. It seems it makes no difference if the timesteps are higher than 50000 (maybe the environment has no bananas left then). Raising `max_t` does not seem to reduce the performance (speed) but increasing the gains per episode.

`n_episode` doesn't make a difference, because I won't reach the limit with my training.

All other parameters kept untouched, because there seemed to be no need. Maybe in a later training I will experiment with these parameters to see if they have an relevant impact for this environment.

### DQN Hyper Parameters

- `BUFFER_SIZE (int)`: replay buffer size
- `BATCH_SIZE (int)`: mini batch size
- `GAMMA (float)`: discount factor
- `TAU (float)`: soft update of target parameters
- `LR (float)`: learning rate for optimizer
- `UPDATE_EVERY (int)`: how often to update the network in steps

Setup:
- `BUFFER_SIZE = int(1e5)`, 
- `BATCH_SIZE = 64`, 
- `GAMMA = 0.99`, 
- `TAU = 1e-3`, 
- `LR = 0.0001`
- `UPDATE_EVERY = 2`

Neural Network Setup:
- 2 Hidden Layer Dense Network Layers, each with RELU activation
- Adam Optimizer


Most of the parameters I left untouched from Lunar Lander, including the NN architecture. The architecture seemed fine for this simple problem (only 37 features, with not such complicated dependencies). Maybe it would be enough to use only a single hidden layer.

`BATCH_SIZE` influences the update rate of the neural network. At higher values the network will train faster but lose some accuracy because the network won't be able to update to outliers (if there are any).
`LR` defines the learning rate for the backpropagation in the neural network training. I started at 0.0005 and reduced it (by 0.0001 per step) down to 0.0001 because the training was oscillating (thus I expected the gradient descent to overshoot the optimal minimum). The training still seems too noisy (DRLN plot) - in a later test I want to check the DL part (loss) to see if L2 regularization will help to get better results. Xavier initialization could help as well because the DL could converge faster.
The network itself was trained without a final activation because the action values should be evaluated.

The network has an initial dimension the same as the state size. For now I did not tune the network, because pytorch seems to lack something similar as tensorboard. The performence values of the network would be interessting, so I could see in case of an unstable training if the problem lies in the RL or the DL part.

I reduced the `UPDATE_EVERY` to 2 because the network will then faster learn due more updates. Update every 3 will increase the needed episodes by 100.


## Performance plot

See: `rewards_plot.png` included in this repository.

```
Episode 100	Average Score: 4.24
Episode 200	Average Score: 10.37
Episode 235	Average Score: 13.02
Environment solved in 135 episodes!	Average Score: 13.02
```


## Improvements

* Set update Steps = 2 for maybe faster converge?  --> done, works better by 100 episodes
* At higher episodes the training seemed unstable. Check if it was a NN or RL problem
* Install on my server when I have the time
* Replace pytorch and plugin in tensorflow to use tensorboard to get some idea about the NN training
* Try Xavier initialization and AdaGrad for NN --> (Adam done, working)
* More tests with enabled trendline I see what the maximum score will be
* Is there a way to separate DQN hyper parameter search from DL hyper parameter search to simplify the search?
* Train and compare to Rainbow agent [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/pdf/1710.02298.pdf).